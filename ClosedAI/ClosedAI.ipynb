{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClosedAI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-earbJvpYK2"
      },
      "source": [
        "#FNC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI_r0TsgmblW"
      },
      "source": [
        "## Dataset Loding and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EFo8yJz9-Vc"
      },
      "source": [
        "import pandas as pd\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Flatten, Input, Dropout, MaxPooling1D, Convolution1D\n",
        "from keras.layers import LSTM, Lambda, merge, Masking\n",
        "from keras.layers import Embedding, TimeDistributed\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from keras import backend as K\n",
        "import keras.callbacks\n",
        "import sys\n",
        "import os\n",
        "import theano as theano\n",
        "import theano.tensor as T\n",
        "from theano.gradient import grad_clip\n",
        "import time\n",
        "import operator\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCo_SdXRmD9i",
        "outputId": "eddea519-46b7-48d9-85c8-bd5a3f8a2782"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm4JwLUg8-LW",
        "outputId": "b64cdc0a-d43b-4425-b96a-0ab9f1e2e08b"
      },
      "source": [
        "# Mount data from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmSUveBs_y7d"
      },
      "source": [
        "path=\"/content/drive/MyDrive/IIITH- Folder/SMAI/SMAI_Project/FNC dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ8duvlY-yEf"
      },
      "source": [
        "train_body = pd.read_csv(\"/content/drive/MyDrive/IIITH- Folder/SMAI/SMAI_Project/FNC dataset/train_bodies.csv\")\n",
        "train_stances = pd.read_csv(\"/content/drive/MyDrive/IIITH- Folder/SMAI/SMAI_Project/FNC dataset/train_stances.csv\")\n",
        "test_body = pd.read_csv(\"/content/drive/MyDrive/IIITH- Folder/SMAI/SMAI_Project/FNC dataset/competition_test_bodies.csv\")\n",
        "test_stances = pd.read_csv(\"/content/drive/MyDrive/IIITH- Folder/SMAI/SMAI_Project/FNC dataset/competition_test_stances.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8CpfvU2CdDt",
        "outputId": "7f77b7bd-68ad-4acb-de35-f24b4e215430"
      },
      "source": [
        "train_body\n",
        "train_body_copy = train_body.copy()\n",
        "train_body_col = train_body.columns\n",
        "train_body_col"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Body ID', 'articleBody'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaa3ynsSCU2F",
        "outputId": "fa04ee0b-ed8f-48ed-b657-341a9e1848cd"
      },
      "source": [
        "train_stances\n",
        "train_stances_copy = train_stances.copy()\n",
        "train_stances_col = train_stances.columns\n",
        "train_stances_col"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Headline', 'Body ID', 'Stance'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2vctuSxM1eQ",
        "outputId": "100a0075-2cab-4de3-cb61-7cd45d8a38b3"
      },
      "source": [
        "print(train_stances.shape)\n",
        "print(train_body.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(49972, 3)\n",
            "(1683, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGrDDqrnQvP5"
      },
      "source": [
        "stances_count = train_stances['Body ID'].unique()\n",
        "body_count = train_body['Body ID'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU70841UYMDP",
        "outputId": "37d6590a-1d05-418a-a315-d0c057ab9c15"
      },
      "source": [
        "count = 0\n",
        "for i in stances_count:\n",
        "    if i not in body_count:\n",
        "        count += 1\n",
        "\n",
        "count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX3cqFdrMgku"
      },
      "source": [
        "body_stance_merged = pd.merge(train_body_copy, train_stances_copy, on='Body ID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz2kL8NaZn83",
        "outputId": "e84cbfae-6fa3-4256-a55b-02965752d7ac"
      },
      "source": [
        "body_stance_merged.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49972, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8YzWJHWCjyH"
      },
      "source": [
        "test_body_copy = test_body\n",
        "test_stances_copy = test_stances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "XiBpjcTx1fmm",
        "outputId": "c2aa5b28-e5a7-4545-84d6-38be5e55cf90"
      },
      "source": [
        "test_body_copy.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Al-Sisi has denied Israeli reports stating tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>CNBC is reporting Tesla has chosen Nevada as t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12</td>\n",
              "      <td>A 4-inch version of the iPhone 6 is said to be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19</td>\n",
              "      <td>GR editor’s Note\\n\\nThere are no reports in th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID                                        articleBody\n",
              "0        1  Al-Sisi has denied Israeli reports stating tha...\n",
              "1        2  A bereaved Afghan mother took revenge on the T...\n",
              "2        3  CNBC is reporting Tesla has chosen Nevada as t...\n",
              "3       12  A 4-inch version of the iPhone 6 is said to be...\n",
              "4       19  GR editor’s Note\\n\\nThere are no reports in th..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "J5d-dZmi1nwL",
        "outputId": "4e3c0904-1384-45e1-931b-375c6abb6169"
      },
      "source": [
        "test_stances_copy.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
              "      <td>2008</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
              "      <td>1550</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
              "      <td>2</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
              "      <td>1793</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
              "      <td>37</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  Body ID     Stance\n",
              "0  Ferguson riots: Pregnant woman loses eye after...     2008  unrelated\n",
              "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550  unrelated\n",
              "2  A Russian Guy Says His Justin Bieber Ringtone ...        2  unrelated\n",
              "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793  unrelated\n",
              "4  Argentina's President Adopts Boy to End Werewo...       37  unrelated"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC4rOm9c43ul",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "f620bcde-c1e7-42c8-e705-196e75155e7c"
      },
      "source": [
        "body_stance_merged"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49967</th>\n",
              "      <td>2532</td>\n",
              "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
              "      <td>Pizza delivery man gets tipped more than $2,00...</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49968</th>\n",
              "      <td>2532</td>\n",
              "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
              "      <td>Pizza delivery man gets $2,000 tip</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49969</th>\n",
              "      <td>2532</td>\n",
              "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
              "      <td>Luckiest Pizza Delivery Guy Ever Gets $2,000 Tip</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49970</th>\n",
              "      <td>2532</td>\n",
              "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
              "      <td>Ann Arbor pizza delivery driver surprised with...</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49971</th>\n",
              "      <td>2532</td>\n",
              "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
              "      <td>Ann Arbor pizza delivery driver surprised with...</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49972 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Body ID  ...     Stance\n",
              "0            0  ...  unrelated\n",
              "1            0  ...  unrelated\n",
              "2            0  ...  unrelated\n",
              "3            0  ...  unrelated\n",
              "4            0  ...  unrelated\n",
              "...        ...  ...        ...\n",
              "49967     2532  ...      agree\n",
              "49968     2532  ...      agree\n",
              "49969     2532  ...      agree\n",
              "49970     2532  ...      agree\n",
              "49971     2532  ...      agree\n",
              "\n",
              "[49972 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ta7AovU48v9"
      },
      "source": [
        "train_body_stance_merged_copy, val_body_stance_merged_copy = train_test_split(body_stance_merged,test_size = 0.70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr9yHiiyz7dA"
      },
      "source": [
        "val_body_stance_merged_copy, remaining_data = train_test_split(body_stance_merged,test_size = 0.70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxGNBBjb5Dc-",
        "outputId": "bce308a5-1a8f-4569-f04a-0cc6e925493a"
      },
      "source": [
        "train_body_stance_merged_copy.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14991, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp5ukJlD5G9q",
        "outputId": "27ae7322-ad5b-4ee9-9e36-f04b4a4acf2e"
      },
      "source": [
        "val_body_stance_merged_copy.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14991, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTFqVo9e5Krq"
      },
      "source": [
        "train_body_stance_merged = train_body_stance_merged_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShYfAF9e5O73"
      },
      "source": [
        "val_body_stance_merged = val_body_stance_merged_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7f6Y0enlH2f"
      },
      "source": [
        "train_body_copy_2 = train_body_copy.copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G-_9eib1tXw"
      },
      "source": [
        "test_body_stance_merged = pd.merge(test_body_copy, test_stances_copy, on='Body ID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQkM2PAs2ACM",
        "outputId": "8e29e7ea-98c6-4833-ede7-9a4bfca31efa"
      },
      "source": [
        "test_body_stance_merged.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25413, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czpw6UGt5ghF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c78b4001-c05d-44db-b9b0-fe776517ab4b"
      },
      "source": [
        "\"\"\"\n",
        "test_body_stance_merged\n",
        "train_body_stance_merged\n",
        "val_body_stance_merged\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntest_body_stance_merged\\ntrain_body_stance_merged\\nval_body_stance_merged\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "-jJPPxRy2LXJ",
        "outputId": "9f1ea932-d7d6-46fc-be91-216b619c6ee3"
      },
      "source": [
        "test_body_stance_merged.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Al-Sisi has denied Israeli reports stating tha...</td>\n",
              "      <td>Apple installing safes in-store to protect gol...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Al-Sisi has denied Israeli reports stating tha...</td>\n",
              "      <td>El-Sisi denies claims he'll give Sinai land to...</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Al-Sisi has denied Israeli reports stating tha...</td>\n",
              "      <td>Apple to keep gold Watch Editions in special i...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Al-Sisi has denied Israeli reports stating tha...</td>\n",
              "      <td>Apple Stores to Keep Gold “Edition” Apple Watc...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Al-Sisi has denied Israeli reports stating tha...</td>\n",
              "      <td>South Korean woman's hair 'eaten' by robot vac...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ...     Stance\n",
              "0        1  ...  unrelated\n",
              "1        1  ...      agree\n",
              "2        1  ...  unrelated\n",
              "3        1  ...  unrelated\n",
              "4        1  ...  unrelated\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_t-fpgWob3b"
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcLRvNgMmsq6"
      },
      "source": [
        "##Converting Stance to Categorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h5Rg6i66mLX"
      },
      "source": [
        "def convert_categorical_data(dataset):\n",
        "  stance_dictionary = {'agree':0 , 'discuss':0 , 'unrelated':1, 'disagree':1}\n",
        "  tmp = dataset['Stance'].map(stance_dictionary)\n",
        "  return tmp\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRc994_5yPeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba7db6d-5a52-4db4-9d4b-112c296a71c7"
      },
      "source": [
        "  train_body_stance_merged['Stance'] = convert_categorical_data(train_body_stance_merged)\n",
        "  val_body_stance_merged['Stance'] = convert_categorical_data(val_body_stance_merged)\n",
        "  test_body_stance_merged['Stance'] =convert_categorical_data(test_body_stance_merged)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57VVM16D7j5P"
      },
      "source": [
        "def check_for_nulls(dataset):\n",
        "  return dataset['articleBody'].isnull().values.any()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xhGull7t_O",
        "outputId": "466eba3c-fd26-4860-a90f-7d1d32d2f363"
      },
      "source": [
        "check_for_nulls(train_body_stance_merged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVOb6RK1P7f8",
        "outputId": "38599db3-260f-4c0d-ddc7-1c7bf62e1aa4"
      },
      "source": [
        "val_body_stance_merged['Headline'].isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgDw1V4jqOYr"
      },
      "source": [
        "Steps\n",
        "1. wordToVec\n",
        "2. Convert both Headline and articleBody to vector.\n",
        "3. get hidden vector from BiGRU\n",
        "4. Pass both vectors  to dense layers parallely and  find cosine similarities btw two\n",
        "5. Classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYf7Mwb3uOYV"
      },
      "source": [
        "train_articleBody = train_body_stance_merged[\"articleBody\"]\n",
        "val_articleBody = val_body_stance_merged[\"articleBody\"]\n",
        "test_articleBody = test_body_stance_merged[\"articleBody\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOgLSHZs8fsR"
      },
      "source": [
        "train_Headline = train_body_stance_merged[\"Headline\"]\n",
        "val_Headline = val_body_stance_merged[\"Headline\"]\n",
        "test_Headline = test_body_stance_merged[\"Headline\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13FE7vLJ9MuO"
      },
      "source": [
        "COPY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPsQjmnCNUsy"
      },
      "source": [
        "train_articleBodycp=train_articleBody.copy()\n",
        "train_Headlinecp=train_Headline.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlIk-WUstpm0"
      },
      "source": [
        "##wordToVec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogaBGEG89zqK"
      },
      "source": [
        "def preprocessing(dataset):\n",
        "  # remove stopwords\n",
        "  dataset = dataset.apply(remove_stopwords)\n",
        "  # preprocess\n",
        "  dataset = [simple_preprocess(line, deacc=True) for line in dataset]\n",
        "  porter_stemmer = PorterStemmer()\n",
        "  # stemmed_tokens\n",
        "  tmp = [[porter_stemmer.stem(word) for word in tokens] for tokens in dataset]\n",
        "  return tmp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0pnFzhk-HWC"
      },
      "source": [
        "train_Headlinecp = preprocessing(train_Headlinecp)\n",
        "train_articleBodycp = preprocessing(train_articleBodycp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhZ-TjXtTp5K",
        "outputId": "0d511214-e1e6-4804-a710-506a35941318"
      },
      "source": [
        "print(type(train_Headlinecp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6falirF_axB"
      },
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAt2IbBd_QcW"
      },
      "source": [
        "def train_wordToVec(dataset, filename):\n",
        "  size = 300\n",
        "  window = 3\n",
        "  min_count = 1\n",
        "  workers = 3\n",
        "  sg = 1 \n",
        "  OUTPUT_FOLDER = \"/content/drive/MyDrive/\"\n",
        "  word2vec_model_file1 = OUTPUT_FOLDER + filename + str(size) + '.model'\n",
        "  start_time = time.time()\n",
        "  stemmed_tokens1 = pd.Series(dataset).values\n",
        "  # Train the Word2Vec Model\n",
        "  w2v_model1 = Word2Vec(stemmed_tokens1, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
        "  print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
        "  w2v_model1.save(word2vec_model_file1)\n",
        "\n",
        "  return stemmed_tokens1, word2vec_model_file1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBoseLpzAA_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbb6d57-790a-4a8f-9f73-120f3de4f0fe"
      },
      "source": [
        "stemmed_tokens1, word2vec_model_file1 = train_wordToVec(train_Headlinecp,\"word2vec_1\")\n",
        "stemmed_tokens2, word2vec_model_file2 = train_wordToVec(train_articleBodycp,\"word2vec_2\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to train word2vec model: 2.7957489490509033\n",
            "Time taken to train word2vec model: 76.21452975273132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4nflKAyEdFm"
      },
      "source": [
        "w2v_model1= Word2Vec.load(word2vec_model_file1)\n",
        "w2v_model2= Word2Vec.load(word2vec_model_file2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT8WwnPmPYqm"
      },
      "source": [
        "w2v_model1.wv.save_word2vec_format('myfile1.txt',binary=False)\n",
        "w2v_model2.wv.save_word2vec_format('myfile2.txt',binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikCU4Mhmna6H"
      },
      "source": [
        "##Embedding Matrix of the Training data and Creating Input tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D3lTUl5PYqm"
      },
      "source": [
        "import os\n",
        "def get_embed_index(filename):\n",
        "  embed_index1={}\n",
        "  f= open(os.path.join('',filename),encoding=\"utf-8\")\n",
        "  for line in f:\n",
        "    values =line.split()\n",
        "    word = values[0]\n",
        "    coefs= np.asarray(values[1:])\n",
        "    embed_index1[word]=coefs\n",
        "  f.close()\n",
        "  return embed_index1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kq2ursxWwJl"
      },
      "source": [
        "embed_index1=get_embed_index('myfile1.txt')\n",
        "embed_index2=get_embed_index('myfile2.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clVV8GYqPYqm"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JacWk7ePYqn"
      },
      "source": [
        "def get_embed_matrix(stemmed_tokens, embed_index,type):\n",
        "  tok_obj=Tokenizer()\n",
        "  tok_obj.fit_on_texts(stemmed_tokens)\n",
        "  sequences = tok_obj.texts_to_sequences(stemmed_tokens)\n",
        "\n",
        "  word_index=tok_obj.word_index\n",
        "  print('Found %s unique tokens.'% len(word_index))\n",
        "\n",
        "  if type=='H':\n",
        "    #max_length=max([len(s) for s in stemmed_tokens])\n",
        "    max_length=25\n",
        "  else:\n",
        "    max_length=1500\n",
        "\n",
        "  review_pad =pad_sequences(sequences,maxlen=max_length)\n",
        "\n",
        "  print(\"Shape of review tensor:\",review_pad.shape)\n",
        "  num_words=len(word_index)+1\n",
        "  embed_matrix=np.zeros((num_words,300))\n",
        "\n",
        "  for word,i in word_index.items():\n",
        "    if i > num_words:\n",
        "      continue\n",
        "    embedding_vector=embed_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embed_matrix[i]=embedding_vector\n",
        "  return embed_matrix,num_words,max_length, review_pad\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Pe0yZOnmYE"
      },
      "source": [
        "###Embedding Matrix and Tensor of Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcDjxfHTPYqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651f404d-3c7f-4744-c28a-962b76064c1f"
      },
      "source": [
        "embed_matrix1,num_words1,max_length1,tensor1=get_embed_matrix(stemmed_tokens1,embed_index1,'H')\n",
        "embed_matrix2,num_words2,max_length2, tensor2=get_embed_matrix(stemmed_tokens2, embed_index2,'A')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2554 unique tokens.\n",
            "Shape of review tensor: (14991, 25)\n",
            "Found 14697 unique tokens.\n",
            "Shape of review tensor: (14991, 1500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KbtXHVkFmp3",
        "outputId": "13258c94-d834-4add-d065-e1379c4832a0"
      },
      "source": [
        "tensor2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14991, 1500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuVtEW2N9mdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc098550-def3-4e3f-a44f-ee4edc81a004"
      },
      "source": [
        "print(embed_matrix1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2555, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e659h7ZV9sQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d815b9fd-d465-4c82-c93b-3f44790ddc84"
      },
      "source": [
        "print(embed_matrix2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14698, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN8FqIooGBE7"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, Concatenate, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVv2QRO7G021",
        "outputId": "fb82593f-55a6-4865-d0fb-9d004a30fa57"
      },
      "source": [
        "train_body_stance_merged['Stance']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12375    1\n",
              "38627    0\n",
              "32812    1\n",
              "18713    1\n",
              "24501    1\n",
              "        ..\n",
              "18295    1\n",
              "12267    1\n",
              "36354    1\n",
              "14671    1\n",
              "42101    0\n",
              "Name: Stance, Length: 14991, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKGJZ1lS41Ky",
        "outputId": "f45958dc-40dc-4bfe-bfa4-8e31e73aa450"
      },
      "source": [
        "train_body_stance_merged.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14991, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ2Afsh9UxM8"
      },
      "source": [
        "# print(len(out_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU4ehUMVGWnx"
      },
      "source": [
        "# model.fit(inp_list, out_list.astype(\"float64\"), batch_size = 64,epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3gl7OP2oMMi"
      },
      "source": [
        "##Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyK0k0l9VcKR",
        "outputId": "7cacde14-8499-47c3-99e9-80c4d97452d6"
      },
      "source": [
        "#Embedding Input\n",
        "embed1=Input(shape=(25,))\n",
        "embed2=Input(shape=(1500,))\n",
        "\n",
        "#Embedding Layer\n",
        "embedding_layer1 = Embedding( num_words1,300,embeddings_initializer=Constant(embed_matrix1),input_length=max_length1,trainable=False)(embed1)\n",
        "embedding_layer2 = Embedding( num_words2,300,embeddings_initializer=Constant(embed_matrix2),input_length=max_length2,trainable=False)(embed2)\n",
        "#Bigru layer\n",
        "bigrulayer1=Bidirectional(GRU(units=250, dropout=0.5, recurrent_dropout=0.2))(embedding_layer1)\n",
        "bigrulayer2=Bidirectional(GRU(units=250, dropout=0.5, recurrent_dropout=0.2))(embedding_layer2)\n",
        "#Similarity layer\n",
        "pin_part = Dense(64, activation='sigmoid')(bigrulayer1)\n",
        "sku_part = Dense(64, activation='sigmoid')(bigrulayer2)\n",
        "#Cosine global similarity\n",
        "global_similarity=keras.layers.Dot(axes=1, normalize=True)([pin_part, sku_part])\n",
        "#Prediction\n",
        "probb= Dense(1, activation='softmax')(global_similarity)\n",
        "\n",
        "model1 = Model(inputs=[embed1] + [embed2], outputs=probb)\n",
        "model1.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 25)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1500)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 25, 300)      766500      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1500, 300)    4409400     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 500)          828000      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 500)          828000      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 64)           32064       bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           32064       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1)            0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            2           dot[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 6,896,030\n",
            "Trainable params: 1,720,130\n",
            "Non-trainable params: 5,175,900\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkjb3A0voQqG"
      },
      "source": [
        "### Input and Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK8rjmvdGUlz"
      },
      "source": [
        "inp_list = [tensor1, tensor2]\n",
        "out_list = train_body_stance_merged['Stance']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJaWb50oUpv"
      },
      "source": [
        "### Fitting the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpv1vpMoV3NE",
        "outputId": "faa4b490-d140-4da9-e6f7-7a1882e508f5"
      },
      "source": [
        "model1.fit(inp_list, out_list.astype(\"float64\"), batch_size = 64,epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "235/235 [==============================] - 6443s 27s/step - loss: 0.7067 - accuracy: 0.7439\n",
            "Epoch 2/2\n",
            "235/235 [==============================] - 6723s 29s/step - loss: 0.6327 - accuracy: 0.7462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f47e0b14890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxCAmTPr5hib"
      },
      "source": [
        "##Validation Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UycDwtzQ5PAC"
      },
      "source": [
        "val_articleBodycp=val_articleBody.copy()\n",
        "val_Headlinecp=val_Headline.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQBgsyW5eFE"
      },
      "source": [
        "val_Headlinecp = preprocessing(val_Headlinecp)\n",
        "val_articleBodycp = preprocessing(val_articleBodycp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YetUnakT5gkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5130e83-2894-416a-fd35-8dee4734625d"
      },
      "source": [
        "stemmed_tokens_val1, word2vec_model_file_v1 = train_wordToVec(val_Headlinecp,\"word2vec_v1\")\n",
        "stemmed_tokens_val2, word2vec_model_file_v2 = train_wordToVec(val_articleBodycp,\"word2vec_v2\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to train word2vec model: 3.0138747692108154\n",
            "Time taken to train word2vec model: 89.79340839385986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3ax1ZwJ6Hqw"
      },
      "source": [
        "w2v_model_v1= Word2Vec.load(word2vec_model_file_v1)\n",
        "w2v_model_v2= Word2Vec.load(word2vec_model_file_v2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwjhlfu86Y4b"
      },
      "source": [
        "w2v_model1.wv.save_word2vec_format('myfile_v1.txt',binary=False)\n",
        "w2v_model2.wv.save_word2vec_format('myfile_v2.txt',binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nluGgx0L6fZj"
      },
      "source": [
        "embed_index_v1=get_embed_index('myfile_v1.txt')\n",
        "embed_index_v2=get_embed_index('myfile_v2.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA_v1LUL6jbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c065c14-49bb-4e66-e7ec-6e24ca6b1c91"
      },
      "source": [
        "embed_matrix_v1,num_words_v1,max_length_v1,tensor_v1=get_embed_matrix(stemmed_tokens_val1,embed_index_v1,'H')\n",
        "embed_matrix_v2,num_words_v2,max_length_v2, tensor_v2=get_embed_matrix(stemmed_tokens_val2, embed_index_v2,'A')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2552 unique tokens.\n",
            "Shape of review tensor: (14991, 25)\n",
            "Found 14688 unique tokens.\n",
            "Shape of review tensor: (14991, 1500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3bXwKuT5mIF"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ZtLaYI5nqM"
      },
      "source": [
        "##Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIp-x71XX4ZO"
      },
      "source": [
        "test_articleBodycp=test_articleBody.copy()\n",
        "test_Headlinecp=test_Headline.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_eNq-NjX4ZW"
      },
      "source": [
        "test_Headlinecp = preprocessing(test_Headlinecp)\n",
        "test_articleBodycp = preprocessing(test_articleBodycp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAvKuaWFX4ZW",
        "outputId": "8b073d4c-f774-4c9f-d398-9a4e92bed7d2"
      },
      "source": [
        "stemmed_tokenst1, word2vec_model_filet1 = train_wordToVec(test_Headlinecp[:10000],\"word2vec_t1\")\n",
        "stemmed_tokenst2, word2vec_model_filet2 = train_wordToVec(test_articleBodycp[:10000],\"word2vec_t2\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to train word2vec model: 1.9092988967895508\n",
            "Time taken to train word2vec model: 46.1835412979126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry-IeUuSX4ZX"
      },
      "source": [
        "w2v_modelt1= Word2Vec.load(word2vec_model_filet1)\n",
        "w2v_modelt2= Word2Vec.load(word2vec_model_filet1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj48feBXX4ZX"
      },
      "source": [
        "w2v_model1.wv.save_word2vec_format('myfilet1.txt',binary=False)\n",
        "w2v_model2.wv.save_word2vec_format('myfilet2.txt',binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cc0BNi8X4ZX"
      },
      "source": [
        "embed_indext1=get_embed_index('myfilet1.txt')\n",
        "embed_indext2=get_embed_index('myfilet2.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsoTsY51X4ZX",
        "outputId": "827b09ce-033b-48be-edb7-e88199f16757"
      },
      "source": [
        "embed_matrixt1,num_wordst1,max_lengtht1,tensort1=get_embed_matrix(stemmed_tokenst1,embed_indext1,'H')\n",
        "embed_matrixt2,num_wordst2,max_lengtht2, tensort2=get_embed_matrix(stemmed_tokenst2, embed_indext2,'A')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1695 unique tokens.\n",
            "Shape of review tensor: (10000, 25)\n",
            "Found 7633 unique tokens.\n",
            "Shape of review tensor: (10000, 1500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1Fu6WJwm-4c"
      },
      "source": [
        "out_list = test_body_stance_merged['Stance'][:10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fPMKyfUbSdx"
      },
      "source": [
        "out_listl=out_list.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFLF2-I3pMLH"
      },
      "source": [
        "### Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz_Z7dMfbBNL"
      },
      "source": [
        "petruth_means = model.predict([tensort1,tensort2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5beBxhEahdB"
      },
      "source": [
        "tetruthClass = []\n",
        "predclass = []\n",
        "from sklearn import metrics\n",
        "\n",
        "for i in range(len(petruth_means)):\n",
        "  \n",
        "    if petruth_means[i] >= 0.5:\n",
        "        predclass.append(0)\n",
        "    else:\n",
        "        predclass.append(1)\n",
        "\n",
        "accuracy = metrics.accuracy_score(out_listl,predclass)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUVPk4CCikHr",
        "outputId": "ff581700-69f8-429e-cf3f-6318cf88d653"
      },
      "source": [
        "score = model1.evaluate([tensort1,tensort2], out_list, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.5669732689857483 / Test accuracy: 0.8346999883651733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIEHINm6qb9_"
      },
      "source": [
        "##Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKdxNA0xxoPa"
      },
      "source": [
        "###Experiment1 - Trainset- 15k and Testset- 10k\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DiVrYfhxlpr",
        "outputId": "ddcff86c-85e9-4585-ffc1-5701f4ab3bb8"
      },
      "source": [
        "embed1=Input(shape=(25,))\n",
        "embed2=Input(shape=(1500,))\n",
        " \n",
        "embedding_layer1 = Embedding( num_words1,300,embeddings_initializer=Constant(embed_matrix1),\n",
        "                             input_length=max_length1,trainable=False)(embed1)\n",
        "embedding_layer2 = Embedding( num_words2,300,embeddings_initializer=Constant(embed_matrix2),\n",
        "                             input_length=max_length2,trainable=False)(embed2)\n",
        "bigrulayer1=Bidirectional(GRU(units=250, dropout=0.3, recurrent_dropout=0.5))(embedding_layer1)\n",
        "bigrulayer2=Bidirectional(GRU(units=250, dropout=0.3, recurrent_dropout=0.5))(embedding_layer2)\n",
        " \n",
        "pin_part = Dense(64, activation='relu')(bigrulayer1)\n",
        "sku_part = Dense(64, activation='relu')(bigrulayer2)\n",
        "\n",
        "global_similarity=keras.layers.Dot(axes=1, normalize=True)([pin_part, sku_part])\n",
        "\n",
        "probb= Dense(1, activation='softmax')(global_similarity)\n",
        "\n",
        "model1 = Model(inputs=[embed1] + [embed2], outputs=probb)\n",
        "model1.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 25)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 1500)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 25, 300)      763500      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1500, 300)    4429200     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 500)          828000      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 500)          828000      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 64)           32064       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 64)           32064       bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1)            0           dense_3[0][0]                    \n",
            "                                                                 dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            2           dot_1[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 6,912,830\n",
            "Trainable params: 1,720,130\n",
            "Non-trainable params: 5,192,700\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq41Xh29xlpw",
        "outputId": "2f0330cd-ef38-43a0-a653-f2e08255f952"
      },
      "source": [
        "model1.fit(inp_list, out_list.astype(\"float64\"), batch_size = 64,epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "235/235 [==============================] - 7324s 31s/step - loss: 0.6869 - accuracy: 0.7448\n",
            "Epoch 2/2\n",
            "235/235 [==============================] - 7342s 31s/step - loss: 0.6382 - accuracy: 0.7442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4b793f2d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEc1KtJ369C0",
        "outputId": "c91f5915-432a-4fb3-b85d-783e79ce8813"
      },
      "source": [
        "score = model1.evaluate([tensort1,tensort2], out_list, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.5790790319442749 / Test accuracy: 0.8346999883651733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQGqHcE1Bb8o"
      },
      "source": [
        "###Experiment2 - Trainset- 15k and Testset- 10k\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asQX-g7GBWV4",
        "outputId": "c8b14b42-cb6c-4a15-e0e9-12ed27cdc255"
      },
      "source": [
        "embed1=Input(shape=(25,))\n",
        "embed2=Input(shape=(1500,))\n",
        " \n",
        "embedding_layer1 = Embedding( num_words1,300,embeddings_initializer=Constant(embed_matrix1),input_length=max_length1,trainable=False)(embed1)\n",
        "embedding_layer2 = Embedding( num_words2,300,embeddings_initializer=Constant(embed_matrix2),input_length=max_length2,trainable=False)(embed2)\n",
        "bigrulayer1=Bidirectional(GRU(units=100, dropout=0.5, recurrent_dropout=0.2))(embedding_layer1)\n",
        "bigrulayer2=Bidirectional(GRU(units=100, dropout=0.5, recurrent_dropout=0.2))(embedding_layer2)\n",
        " \n",
        "pin_part = Dense(128, activation='relu')(bigrulayer1)\n",
        "pin_part_1 = Dropout(0.2)(pin_part)\n",
        "pin_part_2 = Dense(64, activation='relu')(pin_part_1)\n",
        "sk_part = Dense(128, activation='relu')(bigrulayer2)\n",
        "sk_part_1 = Dropout(0.2)(sk_part)\n",
        "sk_part_2 = Dense(64, activation='relu')(sk_part_1)\n",
        "\n",
        "global_similarity=keras.layers.Dot(axes=1, normalize=True)([pin_part_2, sk_part_2])\n",
        "\n",
        "probb= Dense(1, activation='softmax')(global_similarity)\n",
        "\n",
        "model1 = Model(inputs=[embed1] + [embed2], outputs=probb)\n",
        "model1.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 25)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(None, 1500)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 25, 300)      767100      input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_13 (Embedding)        (None, 1500, 300)    4406100     input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_12 (Bidirectional (None, 200)          241200      embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_13 (Bidirectional (None, 200)          241200      embedding_13[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 128)          25728       bidirectional_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 128)          25728       bidirectional_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 128)          0           dense_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 128)          0           dense_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 64)           8256        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 64)           8256        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dot_4 (Dot)                     (None, 1)            0           dense_25[0][0]                   \n",
            "                                                                 dense_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 1)            2           dot_4[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 5,723,570\n",
            "Trainable params: 550,370\n",
            "Non-trainable params: 5,173,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16e8rknDBWV6",
        "outputId": "d9d721de-8fab-4e2f-d3c9-70704930cccf"
      },
      "source": [
        "model1.fit(inp_list, out_list.astype(\"float64\"), batch_size = 64,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "235/235 [==============================] - 3103s 13s/step - loss: 0.6861 - accuracy: 0.7565\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 3134s 13s/step - loss: 0.6358 - accuracy: 0.7543\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 3246s 14s/step - loss: 0.6076 - accuracy: 0.7483\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 3206s 14s/step - loss: 0.5882 - accuracy: 0.7493\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 3269s 14s/step - loss: 0.5744 - accuracy: 0.7522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0db6795190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eddUrlmIBWWC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWsyV05nBWWD"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp8i4sjjBWWI",
        "outputId": "6955431b-0d7e-4c9f-ec2c-d3d06f7d6cd4"
      },
      "source": [
        "score = model1.evaluate([tensort1,tensort2], out_list, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.5012915730476379 / Test accuracy: 0.8356666564941406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LQdSLVHsl75"
      },
      "source": [
        "###Experiment3 - On small dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1E51BiCsjn-",
        "outputId": "bce02570-0406-4243-e067-446ecbdd553b"
      },
      "source": [
        "embed1=Input(shape=(25,))\n",
        "embed2=Input(shape=(1500,))\n",
        " \n",
        "embedding_layer1 = Embedding( num_words1,300,embeddings_initializer=Constant(embed_matrix1),input_length=max_length1,trainable=False)(embed1)\n",
        "embedding_layer2 = Embedding( num_words2,300,embeddings_initializer=Constant(embed_matrix2),input_length=max_length2,trainable=False)(embed2)\n",
        "bigrulayer1=Bidirectional(GRU(units=250, dropout=0.3, recurrent_dropout=0.5))(embedding_layer1)\n",
        "bigrulayer2=Bidirectional(GRU(units=250, dropout=0.3, recurrent_dropout=0.5))(embedding_layer2)\n",
        " \n",
        "pin_part = Dense(64, activation='relu')(bigrulayer1)\n",
        "sku_part = Dense(64, activation='relu')(bigrulayer2)\n",
        "\n",
        "global_similarity=keras.layers.Dot(axes=1, normalize=True)([pin_part, sku_part])\n",
        "\n",
        "probb= Dense(1, activation='softmax')(global_similarity)\n",
        "\n",
        "model1 = Model(inputs=[embed1] + [embed2], outputs=probb)\n",
        "model1.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 25)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 1500)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 25, 300)      513900      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1500, 300)    3093900     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 500)          828000      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 500)          828000      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 64)           32064       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 64)           32064       bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1)            0           dense_3[0][0]                    \n",
            "                                                                 dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            2           dot_1[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 5,327,930\n",
            "Trainable params: 1,720,130\n",
            "Non-trainable params: 3,607,800\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlY4TG5qsjoD",
        "outputId": "b0084413-7232-4aa0-dc30-d64ab6b4bab9"
      },
      "source": [
        "model1.fit(inp_list, out_list.astype(\"float64\"), batch_size = 64,epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "16/16 [==============================] - 524s 32s/step - loss: 0.5971 - accuracy: 0.7701\n",
            "Epoch 2/2\n",
            "16/16 [==============================] - 516s 32s/step - loss: 0.5907 - accuracy: 0.7715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f70ea0a0090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgoY3LODsjoE",
        "outputId": "0ed5c3d6-149d-47cc-d226-17348270e921"
      },
      "source": [
        "score = model1.evaluate([tensort1,tensort2], out_list, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.4944207966327667 / Test accuracy: 0.9539999961853027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQOAnKsUqPqf"
      },
      "source": [
        "# Clickbait"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F_v4PdRezHZ"
      },
      "source": [
        "# CLICKBAIT DATASET\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDfMPtlRmkna"
      },
      "source": [
        "\n",
        "  ### Fields in instances.jsonl:\n",
        "  { \n",
        "\n",
        "    \"id\": \"<instance id>\",\n",
        "    \"postTimestamp\": \"<weekday> <month> <day> <hour>:<minute>:<second> <time_offset> <year>\",\n",
        "    \"postText\": [\"<text of the post with links removed>\"],\n",
        "    \"postMedia\": [\"<path to a file in the media archive>\"],\n",
        "    \"targetTitle\": \"<title of target article>\",\n",
        "    \"targetDescription\": \"<description tag of target article>\",\n",
        "    \"targetKeywords\": \"<keywords tag of target article>\",\n",
        "    \"targetParagraphs\": [\"<text of the ith paragraph in the target article>\"],\n",
        "    \"targetCaptions\": [\"<caption of the ith image in the target article>\"]\n",
        "  }\n",
        "  ### Fields in truth_data.jsonl:\n",
        "  {\n",
        "    \n",
        "    \"id\": \"<instance id>\",\n",
        "    \"truthJudgments\": [<number in [0,1]>],\n",
        "    \"truthMean\": <number in [0,1]>,\n",
        "    \"truthMedian\": <number in [0,1]>,\n",
        "    \"truthMode\": <number in [0,1]>,\n",
        "    \"truthClass\": \"clickbait | no-clickbait\"\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUbVil2FHGGM",
        "outputId": "cf14c203-8e71-425a-b633-a426e738f77b"
      },
      "source": [
        "# Mount data from drive\n",
        "from google.colab import drive\n",
        "# drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beJRreP2sSoB"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import os \n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "\n",
        "path2 = \"/content/drive/MyDrive/IIITH- Folder/SMAI/SMAI_Project/clickbait17-validation-170630\"\n",
        "\n",
        "word2vec_model_file = path2 + '/' + 'word2vec_' + str(300) + '.model'\n",
        "\n",
        "nltk_tokeniser = nltk.tokenize.TweetTokenizer()\n",
        "\n",
        "def process_tweet(text):\n",
        "    FLAGS = re.MULTILINE | re.DOTALL\n",
        "\n",
        "    def hashtag(text):\n",
        "        text = text.group()\n",
        "        hashtag_body = text[1:]\n",
        "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
        "        return result\n",
        "\n",
        "    def allcaps(text):\n",
        "        text = text.group()\n",
        "        return text.lower() + \" <allcaps>\"\n",
        "    eyes = r\"[8:=;]\"\n",
        "    nose = r\"['`\\-]?\"\n",
        "\n",
        "    # function so code less repetitive\n",
        "    def re_sub(pattern, repl):\n",
        "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
        "\n",
        "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
        "    text = re_sub(r\"/\",\" / \")\n",
        "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
        "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
        "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
        "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
        "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
        "    text = re_sub(r\"<3\",\"<heart>\")\n",
        "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
        "    text = re_sub(r\"#\\S+\", hashtag)\n",
        "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
        "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
        "\n",
        "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def tokenise(text, with_process=True):\n",
        "    if with_process:\n",
        "        return nltk_tokeniser.tokenize(process_tweet(text).lower())\n",
        "    else:\n",
        "        # return nltk_tokeniser.tokenize(text)\n",
        "        return tweet_ark_tokenize(text.lower())\n",
        "\n",
        "def generate_embeddings(fp):\n",
        "    sentences = []\n",
        "    headline = []\n",
        "    desc = []\n",
        "    id = []\n",
        "    # files = [\"/MyDrive/clickbait17-validation-170630\"]\n",
        "    files = [path2]\n",
        "    for each_fp in files:\n",
        "        with open(os.path.join(each_fp, 'instances.jsonl'), 'rb') as f:\n",
        "            for each_line in f:\n",
        "                each_item = json.loads(each_line.decode('utf-8'))\n",
        "                for each_sentence in each_item[\"id\"]:\n",
        "                    id.append(each_sentence)\n",
        "                if each_item[\"targetTitle\"]:\n",
        "                    headline.append(tokenise(each_item[\"targetTitle\"]))\n",
        "                if each_item[\"targetDescription\"]:\n",
        "                    sentences.append(tokenise(each_item[\"targetDescription\"]))\n",
        "                for each_sentence in each_item[\"targetParagraphs\"]:\n",
        "                    desc.append(tokenise(each_sentence))\n",
        "                for each_sentence in each_item[\"targetCaptions\"]:\n",
        "                    sentences.append(tokenise(each_sentence))\n",
        "    word2vec_model_h = Word2Vec(headline) \n",
        "    word2vec_model_d = Word2Vec(desc) \n",
        "    # word2vec_model.wv.save_word2vec_format(os.path.join(fp, \"s_clickbait.100.txt\"), binary=False)\n",
        "    return word2vec_model_h, word2vec_model_d, headline, desc\n",
        "\n",
        "\n",
        "# def generate_embeddings(fp):\n",
        "#     sentences = []\n",
        "#     # files = [\"/MyDrive/clickbait17-validation-170630\"]\n",
        "#     files = [path2]\n",
        "#     for each_fp in files:\n",
        "#         with open(os.path.join(each_fp, 'truth.jsonl'), 'rb') as f:\n",
        "#             for each_line in f:\n",
        "#                 each_item = json.loads(each_line.decode('utf-8'))\n",
        "#                 for each_sentence in each_item[\"postText\"]:\n",
        "#                     sentences.append(tokenise(each_sentence))\n",
        "#                 if each_item[\"targetTitle\"]:\n",
        "#                     sentences.append(tokenise(each_item[\"targetTitle\"]))\n",
        "#                 if each_item[\"targetDescription\"]:\n",
        "#                     sentences.append(tokenise(each_item[\"targetDescription\"]))\n",
        "#                 for each_sentence in each_item[\"targetParagraphs\"]:\n",
        "#                     sentences.append(tokenise(each_sentence))\n",
        "#                 for each_sentence in each_item[\"targetCaptions\"]:\n",
        "#                     sentences.append(tokenise(each_sentence))\n",
        "#     word2vec_model = Word2Vec(sentences) \n",
        "#     # word2vec_model.wv.save_word2vec_format(os.path.join(fp, \"s_clickbait.100.txt\"), binary=False)\n",
        "#     return word2vec_model\n",
        "\n",
        "word2vec_model_h, word2vec_model_d, headline, desc= generate_embeddings(path2)\n",
        "\n",
        "# word2vec_model_h.wv.save_word2vec_format('h.txt',binary=False)\n",
        "# word2vec_model_d.wv.save_word2vec_format('d.txt',binary=False)\n",
        "\n",
        "\n",
        "# word2vec_model = generate_embeddings(path2)\n",
        "\n",
        "# word2vec_model.wv.save(word2vec_model_file)\n",
        "\n",
        "# print(word2vec_model['small'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msgKZIIc88Zh",
        "outputId": "e5ffc889-9369-4489-eed3-dc7930b57429"
      },
      "source": [
        "\n",
        "\n",
        "# print(headline.shape)\n",
        "# print(desc.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of list using naive method is : 19538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAvx7m0MUBiE",
        "outputId": "ca21e7db-4507-48be-87da-8b1cb536e6c9"
      },
      "source": [
        "similar = word2vec_model.wv.most_similar('small')\n",
        "print(similar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('large', 0.8136820793151855), ('tiny', 0.6736317873001099), ('smaller', 0.633418083190918), ('huge', 0.61185622215271), ('larger', 0.5739832520484924), ('different', 0.561408519744873), ('thin', 0.5400251150131226), ('remote', 0.5382583737373352), ('modest', 0.5186053514480591), ('buildings', 0.4981464743614197)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2utBnjxSDsv"
      },
      "source": [
        "## Get embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edXC7vSAur-d",
        "outputId": "11d86443-998e-4a13-c03d-a41dec345d4c"
      },
      "source": [
        "# print(headline[0], desc[0])\n",
        "# print(headline[:2])\n",
        "print(truth_means[:10])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['‘', 'inexcusable', '’', 'failures', 'in', 'uk', '<allcaps>', '’', 's', 'response', 'to', 'modern', 'slavery', 'leaving', 'victims', 'destitute', 'while', 'abusers', 'go', 'free', ',', 'report', 'warns'], ['donald', 'trump', 'appoints', 'pro-life', 'advocate', 'as', 'assistant', 'secretary', 'of', 'hhs', '<allcaps>', 'for', 'public', 'affairs']]\n",
            "[[0.13333333332], [1.0], [0.46666666664], [0.93333333332], [0.0], [0.06666666666], [0.33333333332], [0.06666666666], [0.3333333333], [0.13333333332]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtUdAXhwdik1"
      },
      "source": [
        "truth_class = []\n",
        "\n",
        "# print(type(truth_means[0][0]))\n",
        "\n",
        "for i in range(len(truth_means)):\n",
        "  \n",
        "    if truth_means[i][0] > 0.5:\n",
        "        truth_class.append(1)\n",
        "    else:\n",
        "        truth_class.append(0)\n",
        "\n",
        "headline_train = headline[:14600]\n",
        "headline_test = headline[14601:]\n",
        "\n",
        "truth_class_train = truth_class[:14600]\n",
        "truth_class_test = truth_class[14601:]\n",
        "\n",
        "desc_train = desc[:14600]\n",
        "desc_test = desc[14601:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBi6tBmYWgGn",
        "outputId": "7ec06745-fc4b-4566-e445-8e2776dc6106"
      },
      "source": [
        "print(len(headline_train))\n",
        "print(len(desc_train))\n",
        "\n",
        "def train_wordToVec(dataset, filename):\n",
        "  size = 300\n",
        "  window = 3\n",
        "  min_count = 1\n",
        "  workers = 3\n",
        "  sg = 1 \n",
        "  OUTPUT_FOLDER = \"/content/drive/MyDrive/\"\n",
        "  word2vec_model_file1 = OUTPUT_FOLDER + filename + str(size) + '.model'\n",
        "  start_time = time.time()\n",
        "  stemmed_tokens1 = pd.Series(dataset).values\n",
        "  # Train the Word2Vec Model\n",
        "  w2v_model1 = Word2Vec(stemmed_tokens1, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
        "  print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
        "  w2v_model1.save(word2vec_model_file1)\n",
        "\n",
        "  return stemmed_tokens1, word2vec_model_file1\n",
        "\n",
        "\n",
        "stemmed_tokens1, word2vec_model_file1 = train_wordToVec(headline_train,\"word2vec_1\")\n",
        "stemmed_tokens2, word2vec_model_file2 = train_wordToVec(desc_train,\"word2vec_2\" )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14600\n",
            "14600\n",
            "Time taken to train word2vec model: 10.01415228843689\n",
            "Time taken to train word2vec model: 20.383568286895752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY8KUy__hO2n"
      },
      "source": [
        "w2v_model1= Word2Vec.load(word2vec_model_file1)\n",
        "w2v_model2= Word2Vec.load(word2vec_model_file2)\n",
        "w2v_model1.wv.save_word2vec_format('h2.txt',binary=False)\n",
        "w2v_model2.wv.save_word2vec_format('d2.txt',binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr0ZdMOiSTkg"
      },
      "source": [
        "import os\n",
        "def get_embed_index(filename):\n",
        "  embed_index1={}\n",
        "  f= open(os.path.join('',filename),encoding=\"utf-8\")\n",
        "  for line in f:\n",
        "    values =line.split()\n",
        "    word = values[0]\n",
        "    coefs= np.asarray(values[1:])\n",
        "    embed_index1[word]=coefs\n",
        "  f.close()\n",
        "  return embed_index1\n",
        "\n",
        "embed_index1=get_embed_index('h2.txt')\n",
        "embed_index2=get_embed_index('d2.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk5__-6AS4Bi"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def get_embed_matrix(stemmed_tokens, embed_index,type):\n",
        "  tok_obj=Tokenizer()\n",
        "  tok_obj.fit_on_texts(stemmed_tokens)\n",
        "  sequences = tok_obj.texts_to_sequences(stemmed_tokens)\n",
        "\n",
        "  word_index=tok_obj.word_index\n",
        "  print('Found %s unique tokens.'% len(word_index))\n",
        "\n",
        "  if type=='H':\n",
        "    #max_length=max([len(s) for s in stemmed_tokens])\n",
        "    max_length=25\n",
        "  else:\n",
        "    max_length=1500\n",
        "\n",
        "  review_pad =pad_sequences(sequences,maxlen=max_length)\n",
        "\n",
        "  print(\"Shape of review tensor:\",review_pad.shape)\n",
        "  num_words=len(word_index)+1\n",
        "  embed_matrix=np.zeros((num_words,300))\n",
        "\n",
        "  for word,i in word_index.items():\n",
        "    if i > num_words:\n",
        "      continue\n",
        "    embedding_vector=embed_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embed_matrix[i]=embedding_vector\n",
        "  return embed_matrix,num_words,max_length, review_pad\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6For-i-egWMN",
        "outputId": "949a7be9-432d-4d01-eca3-0fe093e16c8e"
      },
      "source": [
        "embed_matrix1,num_words1,max_length1,tensor1=get_embed_matrix(stemmed_tokens1,embed_index1,'H')\n",
        "import itertools\n",
        "# my_list = [elem[0] for elem in your_dict.values()]\n",
        "# print(\"\\n\", dict(itertools.islice(embed_index2.items(), 10)), \"\\n\")\n",
        "\n",
        "embed_index2.update({'.': embed_index2.get('.')[3:]})\n",
        "\n",
        "# print(embed_index2.get('.')[1:])\n",
        "# print(embed_index2.get('.')[0])\n",
        "\n",
        "# print(embed_index2.get('.'))\n",
        "\n",
        "embed_matrix2,num_words2,max_length2, tensor2=get_embed_matrix(stemmed_tokens2, embed_index2,'A')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 20298 unique tokens.\n",
            "Shape of review tensor: (14600, 25)\n",
            "Found 28945 unique tokens.\n",
            "Shape of review tensor: (14600, 1500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uBp5Y7WsVjf",
        "outputId": "8bafe5b7-bb2b-4063-b9d4-a620dac3911c"
      },
      "source": [
        "print(tensor1.shape)\n",
        "print(tensor2.shape)\n",
        "print(embed_matrix1.shape)\n",
        "print(embed_matrix2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14600, 25)\n",
            "(14600, 1500)\n",
            "(20299, 300)\n",
            "(28946, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oduvr6MQKde9"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3ftIOrws0D0",
        "outputId": "672cfb15-12aa-4202-adb2-8f7e9f3ffd58"
      },
      "source": [
        "from keras.initializers import Constant\n",
        "\n",
        "embed1=Input(shape=(25,))\n",
        "embed2=Input(shape=(1500,))\n",
        " \n",
        "embedding_layer1 = Embedding( num_words1,300,embeddings_initializer=Constant(embed_matrix1),input_length=max_length1,trainable=False)(embed1)\n",
        "embedding_layer2 = Embedding( num_words2,300,embeddings_initializer=Constant(embed_matrix2),input_length=max_length2,trainable=False)(embed2)\n",
        "bigrulayer1=Bidirectional(GRU(units=250, dropout=0.3, recurrent_dropout=0.5))(embedding_layer1)\n",
        "bigrulayer2=Bidirectional(GRU(units=250, dropout=0.3, recurrent_dropout=0.5))(embedding_layer2)\n",
        " \n",
        "pin_part = Dense(64, activation='relu')(bigrulayer1)\n",
        "sku_part = Dense(64, activation='relu')(bigrulayer2)\n",
        "\n",
        "global_similarity=keras.layers.Dot(axes=1, normalize=True)([pin_part, sku_part])\n",
        "\n",
        "probb= Dense(1, activation='sigmoid')(global_similarity)\n",
        "\n",
        "model1 = Model(inputs=[embed1] + [embed2], outputs=probb)\n",
        "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 25)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1500)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 25, 300)      6089700     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1500, 300)    8683800     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 500)          828000      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 500)          828000      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 64)           32064       bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           32064       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1)            0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            2           dot[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 16,493,630\n",
            "Trainable params: 1,720,130\n",
            "Non-trainable params: 14,773,500\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYrcCbxwtKFt"
      },
      "source": [
        "inp_list = [tensor1, tensor2]\n",
        "# out_list = train_body_stance_merged['Stance'][:1000]\n",
        "out_list = np.array(truth_class_train, dtype=np.float64)\n",
        "\n",
        "checkpoint_path = \"'drive/MyDrive/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model1.fit(inp_list, out_list, batch_size = 64, epochs=2, callbacks=[cp_callback])\n",
        "\n",
        "model1.save('drive/MyDrive/clickmodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWH8gntu79gK"
      },
      "source": [
        "## Applying on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruKiwtDk78uT"
      },
      "source": [
        "stemmed_tokenst1, word2vec_model_filet1 = train_wordToVec(headline_test,\"word2vec_t1\")\n",
        "stemmed_tokenst2, word2vec_model_filet2 = train_wordToVec(desc_test,\"word2vec_t2\" )\n",
        "\n",
        "w2v_modelt1= Word2Vec.load(word2vec_model_filet1)\n",
        "w2v_modelt2= Word2Vec.load(word2vec_model_filet1)\n",
        "\n",
        "w2v_model1.wv.save_word2vec_format('myfilet1.txt',binary=False)\n",
        "w2v_model2.wv.save_word2vec_format('myfilet2.txt',binary=False)\n",
        "\n",
        "embed_indext1=get_embed_index('myfilet1.txt')\n",
        "embed_indext2=get_embed_index('myfilet2.txt')\n",
        "\n",
        "embed_matrixt1,num_wordst1,max_lengtht1,tensort1=get_embed_matrix(stemmed_tokenst1,embed_indext1,'H')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw7caz64_7Uv"
      },
      "source": [
        "embed_indext2.update({'.': embed_indext2.get('.')[3:]})\n",
        "embed_matrixt2,num_wordst2,max_lengtht2, tensort2=get_embed_matrix(stemmed_tokenst2, embed_indext2,'A')\n",
        "\n",
        "out_list2 = np.array(truth_class_test, dtype=np.float64)\n",
        "\n",
        "score = model1.evaluate([tensort1,tensort2], out_list2, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP__jroegCvt"
      },
      "source": [
        "## Alt: Using glove for vectorisation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iss6QpzDBGM6",
        "outputId": "c445311c-1282-4b75-abdc-ae8b8f8bf043"
      },
      "source": [
        "PAD = \"<pad>\"  \n",
        "UNK = \"<unk>\"  \n",
        "nltk_tokeniser = nltk.tokenize.TweetTokenizer()\n",
        "\n",
        "EmbeddingSize = 100\n",
        "glovepath = \"/content/drive/MyDrive/SMAI_Project/glove.6B.zip (Unzipped Files)/\"\n",
        "filepath = \"/content/drive/MyDrive/SMAI_Project/\"\n",
        "\n",
        "def WordEmbeddingLoader(fp, embedding_size):\n",
        "    embedding = []\n",
        "    vocab = []\n",
        "    linenumber=0\n",
        "    with open(fp, 'r', encoding='UTF-8') as f:\n",
        "        for each_line in f:\n",
        "          \n",
        "            linenumber+=1            \n",
        "            row = each_line.split(' ')\n",
        " \n",
        "            if len(row) == 2:\n",
        "                continue\n",
        "            vocab.append(row[0])\n",
        "            if len(row[1:]) != embedding_size:\n",
        "                print (row[0])\n",
        "                print (len(row[1:]))\n",
        "            embedding.append(np.asarray(row[1:], dtype='float32'))\n",
        "   \n",
        "    word2id = dict(zip(vocab, range(2, len(vocab)+2)))\n",
        "    word2id[PAD] = 0\n",
        "    word2id[UNK] = 1\n",
        "\n",
        "    extra_embedding = [np.zeros(embedding_size), np.random.uniform(-0.1, 0.1, embedding_size)]\n",
        "    embedding = np.append(extra_embedding, embedding, 0)\n",
        "    return word2id, embedding,vocab\n",
        "\n",
        "\n",
        "def data_reader(fps, word2id=None, y_len=1):\n",
        "    ids = []\n",
        "    post_texts = []\n",
        "    post_text_lens = []\n",
        "    truth_means = []\n",
        "    truth_classes = []\n",
        "    id2truth_class = {}\n",
        "    id2truth_mean = {}\n",
        "    for fp in fps:\n",
        "        if y_len:\n",
        "            with open(os.path.join(fp, 'truth.jsonl'), 'rb') as fin:\n",
        "                for each_line in fin:\n",
        "                    each_item = json.loads(each_line.decode('utf-8'))\n",
        "                    if y_len == 4:\n",
        "                        each_label = [0, 0, 0, 0]\n",
        "                        \n",
        "                        for each_key, each_value in Counter(each_item[\"truthJudgments\"]).items():\n",
        "                            each_label[int(each_key//0.3)] = float(each_value)/5\n",
        "                        id2truth_class[each_item[\"id\"]] = each_label\n",
        "                        if each_item[\"truthClass\"] != \"clickbait\":\n",
        "                            assert each_label[0]+each_label[1] > each_label[2]+each_label[3]\n",
        "                        else:\n",
        "                            assert each_label[0]+each_label[1] < each_label[2]+each_label[3]\n",
        "                    if y_len == 2:\n",
        "                        if each_item[\"truthClass\"] == \"clickbait\":\n",
        "                            id2truth_class[each_item[\"id\"]] = [1, 0]\n",
        "                        else:\n",
        "                            id2truth_class[each_item[\"id\"]] = [0, 1]\n",
        "                    if y_len == 1:\n",
        "                        if each_item[\"truthClass\"] == \"clickbait\":\n",
        "                            id2truth_class[each_item[\"id\"]] = [1]\n",
        "                        else:\n",
        "                            id2truth_class[each_item[\"id\"]] = [0]\n",
        "                    id2truth_mean[each_item[\"id\"]] = [float(each_item[\"truthMean\"])]\n",
        "        \n",
        "        with open(os.path.join(fp, 'instances.jsonl'), 'rb') as fin:\n",
        "            for each_line in fin:\n",
        "                each_item = json.loads(each_line.decode('utf-8'))\n",
        "                if each_item[\"id\"] not in id2truth_class and y_len:\n",
        "                    num += 1\n",
        "                    continue\n",
        "                ids.append(each_item[\"id\"])\n",
        "                each_post_text = \" \".join(each_item[\"postText\"])\n",
        "                each_target_description = each_item[\"targetTitle\"]\n",
        "                if y_len:\n",
        "                    truth_means.append(id2truth_mean[each_item[\"id\"]])\n",
        "                    truth_classes.append(id2truth_class[each_item[\"id\"]])\n",
        "                if word2id:\n",
        "                    if (each_post_text+\" \").isspace():\n",
        "                        #the id of <unk>\n",
        "                        post_texts.append([0])\n",
        "                        post_text_lens.append(1)\n",
        "                    else:\n",
        "                        each_post_tokens = tokeniser(each_post_text)\n",
        "                        post_texts.append([word2id.get(each_token, 1) for each_token in each_post_tokens])\n",
        "                        post_text_lens.append(len(each_post_tokens))\n",
        "                else:\n",
        "                    post_texts.append([each_post_text])\n",
        "    return ids, post_texts, truth_classes, post_text_lens, truth_means\n",
        "\n",
        "\n",
        "def Sequence_pader(sequences, maxlen):\n",
        "    if maxlen <= 0:\n",
        "        return sequences\n",
        "    shape = (len(sequences), maxlen)\n",
        "    padded_sequences = np.full(shape, 0)\n",
        "    for i, each_sequence in enumerate(sequences):\n",
        "        if len(each_sequence) > maxlen:\n",
        "            padded_sequences[i] = each_sequence[:maxlen]\n",
        "        else:\n",
        "            padded_sequences[i, :len(each_sequence)] = each_sequence\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "\n",
        "def tokeniser(text, with_process=True):\n",
        "    if with_process:\n",
        "        return nltk_tokeniser.tokenize(tweet_processor(text).lower())\n",
        "        print(\"ar\")\n",
        "    else:\n",
        "        return simpleTokenize(squeezeWhitespace(text.lower()))\n",
        "        print(\"br\")\n",
        "\n",
        "def tweet_processor(text):\n",
        "    FLAGS = re.MULTILINE | re.DOTALL\n",
        "    \n",
        "    def megasplit(pattern, string):\n",
        "        splits = list((m.start(), m.end()) for m in re.finditer(pattern, string))\n",
        "        starts = [0] + [i[1] for i in splits]\n",
        "        ends = [i[0] for i in splits] + [len(string)]\n",
        "        return [string[start:end] for start, end in zip(starts, ends)]\n",
        "    \n",
        "    def hashtag(text):\n",
        "        text = text.group()\n",
        "        hashtag_body = text[1:]\n",
        "        #print(hashtag_body)\n",
        "        \n",
        "        #result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
        "        result = \" \".join([\"<hashtag>\"] + megasplit(r\"(?=[A-Z])\", hashtag_body))\n",
        "        return result\n",
        "\n",
        "    def allcaps(text):\n",
        "        text = text.group()\n",
        "        return text.lower() + \" <allcaps>\"\n",
        "    eyes = r\"[8:=;]\"\n",
        "    nose = r\"['`\\-]?\"\n",
        "\n",
        "    # function so code less repetitive\n",
        "    def re_sub(pattern, repl):\n",
        "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
        "\n",
        "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
        "    text = re_sub(r\"/\",\" / \")\n",
        "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
        "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
        "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
        "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
        "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
        "    text = re_sub(r\"<3\",\"<heart>\")\n",
        "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
        "    text = re_sub(r\"#\\S+\", hashtag)\n",
        "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
        "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
        "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(81)\n",
        "word2id, embedding_matrix,vocab = WordEmbeddingLoader(fp=os.path.join(glovepath, \"glove.6B.\"+str(EmbeddingSize)+\"d.txt\"), embedding_size=EmbeddingSize)\n",
        "\n",
        "# import itertools\n",
        "# print(\"start\")\n",
        "# print(\"\\n\", dict(itertools.islice(test_dict.items(), 10)), type(word2id), \"\\n\")\n",
        "# print(embedding_matrix[:10], type(embedding_matrix), \"\\n\")\n",
        "# print(vocab[:10], type(vocab), \"\\n\")\n",
        "# print(\"end\")\n",
        "\n",
        "with open(os.path.join(filepath, 'word2id.json'), 'w') as fout:\n",
        "    json.dump(word2id, fp=fout)\n",
        "    \n",
        "\n",
        "ids, post_texts, truth_classes, post_text_lens, truth_means = data_reader(word2id=word2id, fps=[os.path.join(filepath, 'clickbait17-validation-170630')], y_len=4)\n",
        "\n",
        "# print(type(ids), type(post_texts), type(post_text_lens), type(target_descriptions))\n",
        "# print(ids[0], post_texts[0], post_text_lens[0], truth_means[0], target_descriptions[0], target_description_lens[0])\n",
        "# 858462320779026433 [2048, 1, 3073, 1536, 1155, 6, 1196, 8177, 1222, 1218, 25254, 112, 26971, 244, 417] 15 [0.13333333332] [] 0\n",
        "# print(\"\\n\", len(ids), len(post_texts), len(post_text_lens), len(target_descriptions), \"\\n\") \n",
        "# 19538  14600\n",
        "ids_train = ids [:14600]\n",
        "ids_test = ids [14601:]\n",
        "post_texts_train = post_texts[:14600]\n",
        "post_texts_test = post_texts[14601:]\n",
        "truth_classes_train = truth_classes[:14600]\n",
        "truth_classes_test = truth_classes[14601:]\n",
        "post_text_lens_train = post_text_lens[:14600]\n",
        "post_text_lens_test = post_text_lens[14601:]\n",
        "truth_means_train = truth_means[:14600]\n",
        "truth_means_test = truth_means[14601:]\n",
        "\n",
        "# print(ids_train[0])\n",
        "\n",
        "# print(\"\\n\", len(ids1), len(ids2), \"\\n\")\n",
        "\n",
        "post_texts_train = np.array(post_texts_train)\n",
        "truth_classes_train = np.array(truth_classes_train)\n",
        "post_text_lens_train = np.array(post_text_lens_train)\n",
        "truth_means_train = np.array(truth_means_train)\n",
        "shuffle_indices = np.random.permutation(np.arange(len(post_texts_train)))\n",
        "post_texts_train = post_texts_train[shuffle_indices]\n",
        "truth_classes_train = truth_classes_train[shuffle_indices]\n",
        "post_text_lens_train = post_text_lens_train[shuffle_indices]\n",
        "truth_means_train = truth_means_train[shuffle_indices]\n",
        "max_post_text_len = max(post_text_lens_train)\n",
        "# print(max_post_text_len)\n",
        "\n",
        "post_texts_train = Sequence_pader(post_texts_train, max_post_text_len)\n",
        "\n",
        "post_texts_test = np.array(post_texts_test)\n",
        "truth_classes_test = np.array(truth_classes_test)\n",
        "post_text_lens_test = [each_len if each_len <= max_post_text_len else max_post_text_len for each_len in post_text_lens_test]\n",
        "post_text_lens_test = np.array(post_text_lens_test)\n",
        "truth_means_test = np.array(truth_means_test)\n",
        "truth_means_test = np.ravel(truth_means_test).astype(np.float32)\n",
        "post_texts_test = Sequence_pader(post_texts_test, max_post_text_len)\n",
        "\n",
        "max_features = len(word2id.keys())\n",
        "maxlen = max_post_text_len\n",
        "EmbeddingSize = 100\n",
        "dropout_embedding = 0.2\n",
        "X_train = post_texts_train\n",
        "y_train = truth_means_train\n",
        "X_test = post_texts_test\n",
        "y_test = truth_means_test\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:201: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:215: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrStzeZzBKPw"
      },
      "source": [
        "## Glove Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUuiWOa3gRdN",
        "outputId": "2d709a89-7ff7-4902-f8f7-610db545b1b7"
      },
      "source": [
        "# build the keras LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim = max_features, output_dim = EmbeddingSize, weights = [embedding_matrix], input_length = maxlen ,\n",
        "                    trainable = False))\n",
        "model.add(Dropout(dropout_embedding))\n",
        "model.add(Bidirectional(GRU(512, dropout=0.2, recurrent_dropout=0.5)))  \n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss='mse', optimizer='rmsprop')\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "earlystop_cb = keras.callbacks.EarlyStopping(monitor='mse', patience=7, verbose=1, mode='auto')\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=20, validation_split=0.1, callbacks=[earlystop_cb])\n",
        "\n",
        "petruth_means = model.predict(X_test)\n",
        "tetruthClass = []\n",
        "petruthClass = []\n",
        "\n",
        "\n",
        "for i in range(len(truth_means_test)):\n",
        "  \n",
        "    if petruth_means[i] > 0.5:\n",
        "        petruthClass.append(1)\n",
        "    else:\n",
        "        petruthClass.append(0)\n",
        "    \n",
        "    if truth_means_test[i] > 0.5:\n",
        "        tetruthClass.append(1)\n",
        "    else:\n",
        "        tetruthClass.append(0)\n",
        "\n",
        "\n",
        "mse = metrics.mean_squared_error(truth_means_test, petruth_means)\n",
        "print('Mean Squared Error = '+str(mse))\n",
        "\n",
        "accuracy = metrics.accuracy_score(tetruthClass,petruthClass)\n",
        "print('accuracy = '+str(accuracy))\n",
        "\n",
        "precision = metrics.precision_score(tetruthClass,petruthClass)\n",
        "print('precision_score = '+str(precision))\n",
        "\n",
        "recall = metrics.recall_score(tetruthClass,petruthClass)\n",
        "print('recall_score = '+str(recall))\n",
        "\n",
        "f1 = metrics.f1_score(tetruthClass,petruthClass)\n",
        "print('f1_score = '+str(f1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:201: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:215: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/20\n",
            "206/206 [==============================] - 59s 253ms/step - loss: 0.0517 - val_loss: 0.0377\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 2/20\n",
            "206/206 [==============================] - 51s 248ms/step - loss: 0.0373 - val_loss: 0.0325\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 3/20\n",
            "206/206 [==============================] - 51s 247ms/step - loss: 0.0347 - val_loss: 0.0358\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 4/20\n",
            "206/206 [==============================] - 52s 251ms/step - loss: 0.0329 - val_loss: 0.0302\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 5/20\n",
            "206/206 [==============================] - 51s 248ms/step - loss: 0.0322 - val_loss: 0.0321\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 6/20\n",
            "206/206 [==============================] - 51s 250ms/step - loss: 0.0314 - val_loss: 0.0280\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 7/20\n",
            "206/206 [==============================] - 51s 248ms/step - loss: 0.0298 - val_loss: 0.0311\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 8/20\n",
            "206/206 [==============================] - 52s 251ms/step - loss: 0.0299 - val_loss: 0.0287\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 9/20\n",
            "206/206 [==============================] - 51s 248ms/step - loss: 0.0276 - val_loss: 0.0312\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 10/20\n",
            "206/206 [==============================] - 51s 246ms/step - loss: 0.0269 - val_loss: 0.0290\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 11/20\n",
            "206/206 [==============================] - 51s 246ms/step - loss: 0.0265 - val_loss: 0.0311\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 12/20\n",
            "206/206 [==============================] - 51s 249ms/step - loss: 0.0251 - val_loss: 0.0293\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 13/20\n",
            "206/206 [==============================] - 52s 251ms/step - loss: 0.0248 - val_loss: 0.0298\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 14/20\n",
            "206/206 [==============================] - 51s 247ms/step - loss: 0.0237 - val_loss: 0.0265\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 15/20\n",
            "206/206 [==============================] - 51s 249ms/step - loss: 0.0231 - val_loss: 0.0277\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 16/20\n",
            "206/206 [==============================] - 51s 249ms/step - loss: 0.0216 - val_loss: 0.0275\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 17/20\n",
            "206/206 [==============================] - 51s 249ms/step - loss: 0.0217 - val_loss: 0.0288\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 18/20\n",
            "206/206 [==============================] - 51s 247ms/step - loss: 0.0206 - val_loss: 0.0284\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 19/20\n",
            "206/206 [==============================] - 51s 249ms/step - loss: 0.0199 - val_loss: 0.0295\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Epoch 20/20\n",
            "206/206 [==============================] - 51s 248ms/step - loss: 0.0194 - val_loss: 0.0274\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `mse` which is not available. Available metrics are: loss,val_loss\n",
            "Mean Squared Error = 0.031406514\n",
            "accuracy = 0.8553777597731416\n",
            "precision_score = 0.745158002038736\n",
            "recall_score = 0.6117154811715482\n",
            "f1_score = 0.671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyVQ9lprZYF6",
        "outputId": "f9325a47-789f-4528-8e35-2321f3adce10"
      },
      "source": [
        "print(truth_means[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.13333333332], [1.0], [0.46666666664], [0.93333333332], [0.0], [0.06666666666], [0.33333333332], [0.06666666666], [0.3333333333], [0.13333333332]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKW_8oW2GsGx"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "# del model  # deletes the existing model\n",
        "\n",
        "# returns a compiled model identical to the previous one\n",
        "# model = load_model('my_model.h5')\n",
        "\n",
        "# import pickle\n",
        "# import joblib\n",
        "  \n",
        "# # save the model to disk\n",
        "# filename = 'finalized_model.sav'\n",
        "# pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "# joblib.dump(model, filename) \n",
        "# # load the model from disk\n",
        "# loaded_model = pickle.load(open(filename, 'rb'))\n",
        "# result = loaded_model.score(X_test, Y_test)\n",
        "# print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ODwZdKCeiiT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8sYoh8cekiY"
      },
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# baseFolderPath = path2\n",
        "\n",
        "# fileNames, filePathList = returnListOfFilePaths(baseFolderPath)\n",
        "\n",
        "# rawContentDict = create_docContentDict(filePathList)\n",
        "\n",
        "# # calculate tfidf\n",
        "# tfidf = TfidfVectorizer(tokenizer=processData, 'english') #stop_words\n",
        "# tfs = tfidf.fit_transform(rawContentDict.values())\n",
        "\n",
        "\n",
        "# # TODO: modify this to build matrix then print from matrix form\n",
        "# def calc_and_print_CosineSimilarity_for_all(tfs, fileNames):\n",
        "#     #print(cosine_similarity(tfs[0], tfs[1]))\n",
        "#     numFiles = len(fileNames)\n",
        "#     names = []\n",
        "#     print('                   ', end=\"\")    #formatting\n",
        "#     for i in range(numFiles):\n",
        "#         if i == 0:\n",
        "#             for k in range(numFiles):\n",
        "#                 print(fileNames[k], end='   ')\n",
        "#             print()\n",
        "\n",
        "#         print(fileNames[i], end='   ')\n",
        "#         for n in range(numFiles):\n",
        "#             #print(fileNames[n], end='\\t')\n",
        "#             matrixValue = cosine_similarity(tfs[i], tfs[n])\n",
        "#             numValue = matrixValue[0][0]\n",
        "#             #print(numValue, end='\\t')\n",
        "#             names.append(fileNames[n])\n",
        "#             print(\" {0:.8f}\".format(numValue), end='         ')\n",
        "#             #(cosine_similarity(tfs[i], tfs[n]))[0][0]\n",
        "\n",
        "#         print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzzrGfqGLoJg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9jfN44DLmUd"
      },
      "source": [
        "# class MyBasicAttentiveBiGRU(models.Model):\n",
        "\n",
        "#     def __init__(self, vocab_size: int, embed_dim: int, hidden_size: int = 128, training: bool = False):\n",
        "#         super(MyBasicAttentiveBiGRU, self).__init__()\n",
        "\n",
        "#         self.num_classes = len(ID_TO_CLASS)\n",
        "\n",
        "#         self.decoder = layers.Dense(units=self.num_classes)\n",
        "#         self.omegas = tf.Variable(tf.random.normal((hidden_size*2, 1)))\n",
        "\n",
        "#         ### TODO(Students) START\n",
        "#         self.embeddings = tf.Variable(tf.random.normal((vocab_size, embed_dim)), trainable = training) \n",
        "        \n",
        "#         self._forward_layer = layers.GRU(hidden_size, return_sequences=True)\n",
        "#         self._backward_layer = layers.GRU(hidden_size, return_sequences=True, go_backwards=True)\n",
        "#         self._bidirectional_layer = layers.Bidirectional(self._forward_layer, backward_layer=self._backward_layer, merge_mode = 'concat')\n",
        "#         ### TODO(Students) END\n",
        "\n",
        "#     def attn(self, rnn_outputs):\n",
        "#         ### TODO(Students) START\n",
        "\n",
        "#         # The following lines are based on the equation 9-12 by Zhou et al.\n",
        "        \n",
        "#         # Equation 9: M = tanh(H)\n",
        "#         M = tf.tanh(rnn_outputs) # 10, 5, 256\n",
        "\n",
        "#         # Equation 10: \\alpha = softmax(w^T.M)\n",
        "#         alpha = tf.nn.softmax(tf.tensordot(M, self.omegas, axes=1))  # 10, 5, 1\n",
        "\n",
        "#         # Equation 11: H.\\alpha^T\n",
        "#         r = tf.reduce_sum(rnn_outputs * alpha, axis=1)\n",
        "\n",
        "#         # Equation 12: tanh(r)\n",
        "#         output = tf.tanh(r)\n",
        "\n",
        "#         ### TODO(Students) END\n",
        "\n",
        "#         return output\n",
        "\n",
        "#     def call(self, inputs, pos_inputs, training):\n",
        "        \n",
        "#         word_embed = tf.nn.embedding_lookup(self.embeddings, inputs) # 10, 5, 100\n",
        "#         pos_embed = tf.nn.embedding_lookup(self.embeddings, pos_inputs) # 10, 5, 100\n",
        "        \n",
        "#         ### TODO(Students) START\n",
        "\n",
        "#         # First, we will concatenate the word and POS embeddings along the 3rd axis\n",
        "#         input_for_bidirectional = tf.concat([word_embed, pos_embed], axis=2) # 10, 5, 200\n",
        "        \n",
        "#         # input_for_bidirectional = word_embed # This is for experiments, comment the above line\n",
        "\n",
        "#         # Create a mask for the sequence as it is padded with 0\n",
        "#         mask = tf.cast(tf.greater(inputs, 0), tf.float32)\n",
        "\n",
        "#         # Pass the word (+ POS) embeddings to bidirectional GRU layer\n",
        "#         hidden_outputs = self._bidirectional_layer(input_for_bidirectional, mask = mask) # 10, 5, 256\n",
        "\n",
        "#         # Apply attention to the hidden state outputs from forward and backward layer of biGRU\n",
        "#         attn = self.attn(hidden_outputs)\n",
        "\n",
        "#         # Apply the dense layer to obtain logits\n",
        "#         logits = self.decoder(attn)\n",
        "#         ### TODO(Students) END\n",
        "\n",
        "#         # batch_size, num_classes\n",
        "#         return {'logits': logits}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FKgN8Dt5p76"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}